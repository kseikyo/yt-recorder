# yt-recorder v0.2 — Architecture Evolution Plan

## TL;DR

Five-track plan: stabilize (commit fixes), refactor foundation (protocols + enums),
ship features (playlist, no-speech, clean cmd), harden (tests), future-proof
(storage backends, self-host, open-source posture). Each track has dependency-ordered
waves. Total: ~25 tasks across 9 waves.

## Complexity Score: 75/100

| Dimension (25% each) | Score | Why |
|---|---|---|
| Concern count | 9/10 | 9+ distinct concerns spanning bugs, features, architecture, future |
| File count | 7/10 | ~18 files touched (models, adapters, pipeline, CLI, tests, config, docs) |
| Cross-domain | 6/10 | All backend/CLI, but model changes cascade through every layer |
| Interdependency | 8/10 | TranscriptStatus touches 8+ files; protocols touch all adapters |

**Decision: DECOMPOSE** — hybrid strategy (layer + feature boundaries)

## Context

### Current State
- 7 commits on main, 2 files uncommitted (youtube.py upload fixes, test mock cleanup)
- Upload flow works e2e; playlist assignment broken (stale selectors)
- TranscriptUnavailableError creates infinite retry loop (registry never updated)
- No `clean` command; deletion of `--keep` files architecturally impossible
- 2 orphaned galaxy.mp4 uploads on YouTube (no playlist, from debugging)
- 23 youtube tests pass; 0 CLI tests; 0 pipeline transcript tests

### Architecture Today
```
CLI (Click) → Pipeline → Adapters → Domain
                           ├─ youtube.py  (concrete class)
                           ├─ raid.py     (concrete class)
                           ├─ transcriber.py (concrete class)
                           ├─ registry.py (concrete class)
                           └─ scanner.py  (function)
```
Hexagonal structure exists in directory layout but NOT enforced by types.
No Protocol definitions. Pipeline imports concrete adapter classes directly.

### Architecture Target
```
CLI → Pipeline (depends on Protocols) → Domain
         │
    ┌────┼────────┬──────────┬───────────┐
    ▼    ▼        ▼          ▼           ▼
  RAID  YouTube  Transcriber  Registry   Scanner
  (impl) (impl)   (impl)     (impl)    (impl)
                               │
                          ┌────┴────┐
                          ▼         ▼ (future)
                       Markdown   SQLite/JSON
```

## Global Constraints

### Naming & Style
- Python 3.9 target (no `match`, no `type X = ...`, use `from __future__ import annotations`)
- mypy --strict, ruff check, ruff-format
- Commit messages: concise, sacrifice grammar
- Pre-commit: ruff + mypy hooks

### Architecture Rules
- Hexagonal: CLI → Pipeline → Adapters → Domain (never skip layers)
- Domain layer has ZERO adapter imports
- Pipeline depends on Protocol types, not concrete adapter classes
- Adapters implement Protocols via duck typing (no explicit inheritance)
- All YouTube selectors live in constants.py (no inline selectors in adapters)

### Testing
- pytest + unittest.mock
- Adapters tested with mocked externals (Playwright, yt-dlp)
- Pipeline tested with mocked adapter protocols
- CLI tested with click.testing.CliRunner

### Privacy & Security
- All credentials stay local (storage_state.json, cookies.txt)
- No telemetry, no cloud services beyond YouTube itself
- Credential files: 0600 permissions, .gitignore'd
- NEVER log video IDs or account names at INFO level

### Backwards Compatibility
- Registry v1 (✅/❌ booleans) auto-migrated to v2 on first load
- Config.toml changes are additive only (new keys with defaults)

---

## Wave 1 — Stabilize (no architecture changes)

### TODO 1.1: Commit upload flow fixes
- [x] What: `git add` youtube.py + test_youtube.py, commit, push
- [x] Changes: networkidle→domcontentloaded, wait_for_function Done btn, dialog close wait, dead mock removal
- [ ] Must NOT: Include any new changes beyond the 5 existing fixes
- [ ] Acceptance:
  - `git diff --cached` shows exactly the 4 changes described
  - `pytest tests/test_youtube.py` passes
  - `mypy --strict src` passes
  - `ruff check src tests` passes
- [ ] QA: `git log --oneline -1` shows concise commit message
- Parallelization: wave=1, blocks=[], blocked-by=[]
- Commit: `fix: upload flow — domcontentloaded, wait_for_function Done btn, dialog close`

### TODO 1.2: Move playlist selectors to constants.py
- [ ] What: Extract 3 hardcoded selectors from youtube.py assign_playlist() into constants.py
- [ ] Selectors:
  ```python
  # constants.py additions
  PLAYLIST_DROPDOWN = 'tp-yt-paper-button[aria-label*="Playlist"]'
  PLAYLIST_OPTION = "tp-yt-paper-item:has-text('{name}')"  # .format(name=...)
  PLAYLIST_SAVE = "tp-yt-button-shape[aria-label='Save']"
  ```
- [ ] Must NOT: Change selector values yet (they're stale, fixed in Wave 5)
- [ ] Must NOT: Change assign_playlist behavior (still warns, still returns None)
- [ ] Acceptance:
  - `grep -n "tp-yt-paper" src/yt_recorder/adapters/youtube.py` returns 0 results
  - `grep "PLAYLIST_" src/yt_recorder/constants.py` returns 3 lines
  - All 23 tests pass unchanged
- Parallelization: wave=1, blocks=[], blocked-by=[1.1]
- Commit: `refactor: move playlist selectors to constants.py`

---

## Wave 2 — Foundation: Protocols (enables future extensibility)

### TODO 2.1: Create domain/protocols.py with adapter port definitions
- [ ] What: Define Protocol classes that formalize the adapter interfaces
- [ ] Design:
  ```python
  # domain/protocols.py
  from typing import Protocol, runtime_checkable
  from pathlib import Path
  from yt_recorder.domain.models import UploadResult, RegistryEntry

  @runtime_checkable
  class VideoUploader(Protocol):
      def open(self) -> None: ...
      def close(self) -> None: ...
      def upload(self, path: Path, title: str) -> UploadResult: ...
      def assign_playlist(self, video_id: str, playlist_name: str) -> bool: ...

  @runtime_checkable
  class TranscriptFetcher(Protocol):
      def fetch(self, video_id: str, lang: str = ...) -> Path: ...

  @runtime_checkable
  class RegistryStore(Protocol):
      def load(self) -> list[RegistryEntry]: ...
      def append(self, entry: RegistryEntry) -> None: ...
      def update_transcript(self, file: str, status: str) -> None: ...
      def update_account_id(self, file: str, account: str, video_id: str) -> None: ...

  @runtime_checkable
  class FileScanner(Protocol):
      def __call__(
          self, directory: Path, extensions: list[str],
          exclude_dirs: list[str], max_depth: int,
      ) -> list[tuple[Path, str]]: ...
  ```
- [ ] Must NOT: Change any existing adapter code yet
- [ ] Must NOT: Change pipeline imports yet (that's 2.2)
- [ ] Acceptance:
  - `mypy --strict src` passes (Protocol definitions type-check)
  - No existing tests break
  - File exists at src/yt_recorder/domain/protocols.py
- Parallelization: wave=2, blocks=[2.2], blocked-by=[1.1]
- Commit: `feat: define adapter Protocol interfaces in domain layer`

### TODO 2.2: Refactor pipeline.py to depend on Protocols
- [ ] What: Change Pipeline.__init__ type hints from concrete classes to Protocol types
- [ ] Design:
  ```python
  # pipeline.py changes
  from yt_recorder.domain.protocols import RegistryStore, TranscriptFetcher
  # RaidAdapter stays concrete (it's an orchestrator, not a port)
  class RecordingPipeline:
      def __init__(
          self,
          config: Config,
          registry: RegistryStore,  # was: MarkdownRegistryStore
          raid: RaidAdapter,
          transcriber: TranscriptFetcher | None = None,  # was: YtdlpTranscriptAdapter
      ): ...
  ```
- [ ] Must NOT: Change from_directory() factory — it still constructs concrete types
- [ ] Must NOT: Break any existing tests (mocks satisfy protocols via duck typing)
- [ ] Acceptance:
  - `mypy --strict src` passes
  - All existing tests pass without modification
  - `grep "MarkdownRegistryStore" src/yt_recorder/pipeline.py` only in from_directory()
- Parallelization: wave=2, blocks=[], blocked-by=[2.1]
- Commit: `refactor: pipeline depends on Protocol interfaces, not concrete adapters`

### TODO 2.3: Deduplicate _find_chrome()
- [ ] What: Single _find_chrome() in a shared utility, imported by youtube.py and cli.py
- [ ] Location: `src/yt_recorder/utils.py` (new file)
- [ ] Must NOT: Change the function's behavior
- [ ] Acceptance:
  - `grep -rn "def _find_chrome" src/` returns exactly 1 result
  - Both youtube.py and cli.py import from utils
  - Tests pass
- Parallelization: wave=2, blocks=[], blocked-by=[1.1]
- Commit: `refactor: deduplicate _find_chrome into utils module`

---

## Wave 3 — Foundation: TranscriptStatus Enum + Registry Migration

### TODO 3.1: Add TranscriptStatus enum to domain/models.py
- [ ] What: Replace has_transcript: bool with transcript_status: TranscriptStatus
- [ ] Design:
  ```python
  from enum import Enum

  class TranscriptStatus(str, Enum):
      PENDING = "pending"           # Not attempted / not ready yet
      DONE = "done"                 # Successfully fetched
      UNAVAILABLE = "unavailable"   # No captions exist; stop retrying
      ERROR = "error"               # Transient failure; can retry

  @dataclass(frozen=True)
  class RegistryEntry:
      file: str
      playlist: str
      uploaded_date: date
      transcript_status: TranscriptStatus  # was: has_transcript: bool
      account_ids: dict[str, str]

      @property
      def has_transcript(self) -> bool:
          """Backwards-compat property."""
          return self.transcript_status == TranscriptStatus.DONE
  ```
- [ ] Key decision: `str, Enum` base makes it serializable to/from registry markdown
- [ ] Must NOT: Remove has_transcript entirely (keep as property for gradual migration)
- [ ] Acceptance:
  - `TranscriptStatus("pending") == TranscriptStatus.PENDING` → True
  - `entry.has_transcript` still works as bool for existing code
  - mypy --strict passes
- Parallelization: wave=3, blocks=[3.2, 3.3], blocked-by=[2.1]
- Commit: `feat: TranscriptStatus enum replaces has_transcript boolean`

### TODO 3.2: Registry v2 format + auto-migration
- [ ] What: Update MarkdownRegistryStore to read/write TranscriptStatus values
- [ ] Migration logic:
  ```python
  def _parse_transcript_status(self, raw: str) -> TranscriptStatus:
      # v1 migration
      if raw == "✅": return TranscriptStatus.DONE
      if raw == "❌": return TranscriptStatus.PENDING
      # v2 native
      return TranscriptStatus(raw)

  def _format_transcript_status(self, status: TranscriptStatus) -> str:
      return status.value  # "pending", "done", "unavailable", "error"
  ```
- [ ] Registry file gets version comment: `<!-- registry-version: 2 -->`
- [ ] Must NOT: Fail on v1 format (auto-migrate on read)
- [ ] Must NOT: Write v1 format (always write v2)
- [ ] Acceptance:
  - Load a v1 registry → entries have correct TranscriptStatus values
  - Save → file uses v2 format (string values, not emoji)
  - Round-trip: load v2 → save → load → identical entries
  - test_registry.py updated with v1→v2 migration test
- Parallelization: wave=3, blocks=[3.3], blocked-by=[3.1]
- Commit: `feat: registry v2 format with auto-migration from v1`

### TODO 3.3: Update pipeline + status to use TranscriptStatus
- [ ] What: Pipeline fetch_transcripts() sets proper status on each outcome
- [ ] Changes:
  ```python
  # pipeline.py fetch_transcripts()
  except TranscriptNotReadyError:
      pending += 1  # stays PENDING, will be retried
  except TranscriptUnavailableError:
      self.registry.update_transcript(entry.file, TranscriptStatus.UNAVAILABLE)
      errors.append(f"No captions for {entry.file} — marked unavailable")
  except Exception as e:
      self.registry.update_transcript(entry.file, TranscriptStatus.ERROR)
      errors.append(f"Failed: {entry.file}: {e}")
  ```
- [ ] Filter logic: only process entries where transcript_status == PENDING (or ERROR if retry=True)
- [ ] Status command: show transcript_status value instead of binary icon
- [ ] Must NOT: Change upload_new() — it still creates entries with PENDING
- [ ] Acceptance:
  - TranscriptUnavailableError → entry marked UNAVAILABLE → never retried
  - TranscriptNotReadyError → entry stays PENDING → retried next run
  - `--retry` flag also retries ERROR entries
  - `yt-recorder status` shows "pending", "done", "unavailable", "error"
- Parallelization: wave=3, blocks=[], blocked-by=[3.1, 3.2]
- Commit: `feat: pipeline sets TranscriptStatus per outcome, fixes infinite retry`

---

## Wave 4 — Feature: Clean Command

### TODO 4.1: Pipeline.clean_synced() method
- [ ] What: Logic to identify and delete local files that are fully synced
- [ ] Design:
  ```python
  @dataclass(frozen=True)
  class CleanReport:
      deleted: int = 0
      kept: int = 0
      errors: list[str] = field(default_factory=list)
      eligible: list[str] = field(default_factory=list)  # for dry-run

  def clean_synced(
      self, directory: Path, dry_run: bool = False
  ) -> CleanReport:
  ```
- [ ] Eligibility: file exists locally AND all accounts have video_id ≠ "—"
  AND transcript_status in {DONE, UNAVAILABLE}
- [ ] Must NOT: Delete files with transcript_status PENDING or ERROR
- [ ] Must NOT: Delete files missing from any account
- [ ] Acceptance:
  - File with all accounts + transcript DONE → deleted
  - File with all accounts + transcript UNAVAILABLE → deleted
  - File with transcript PENDING → kept
  - File with any account "—" → kept
  - --dry-run → nothing deleted, eligible list populated
  - Tests: ≥5 scenarios covering each combination
- Parallelization: wave=4, blocks=[4.2], blocked-by=[3.3]
- Commit: `feat: clean_synced() deletes fully-synced local files`

### TODO 4.2: CLI clean command
- [ ] What: `yt-recorder clean DIRECTORY [--dry-run]`
- [ ] Design:
  ```python
  @main.command()
  @click.argument("directory", ...)
  @click.option("--dry-run", is_flag=True, help="Show what would be deleted")
  def clean(directory: Path, dry_run: bool) -> None:
  ```
- [ ] Output: list of files deleted (or would be deleted), summary counts
- [ ] Must NOT: Delete without confirmation unless --yes or --dry-run shows first
- [ ] Acceptance:
  - `yt-recorder clean ~/recordings --dry-run` lists eligible files
  - `yt-recorder clean ~/recordings` deletes eligible files, shows count
  - README updated with clean command docs
- Parallelization: wave=4, blocks=[], blocked-by=[4.1]
- Commit: `feat: CLI clean command for post-sync file deletion`

---

## Wave 5 — Feature: Fix Playlist Assignment

### TODO 5.1: Inspect YouTube Studio DOM for current playlist selectors
- [ ] What: Open a real YouTube Studio edit page, inspect actual DOM
- [ ] Method: headful Playwright session to studio.youtube.com/video/{id}/edit
- [ ] Deliverable: Updated constants in constants.py
- [ ] Must NOT: Guess selectors — must verify against live DOM
- [ ] Acceptance:
  - Each selector verified to exist on a real edit page
  - constants.py PLAYLIST_* values updated
  - Document the DOM path in a code comment for future updates
- Parallelization: wave=5, blocks=[5.2], blocked-by=[1.2]
- Commit: `fix: update playlist selectors to match current YouTube Studio DOM`

### TODO 5.2: assign_playlist returns bool + playlist creation
- [ ] What: Refactor assign_playlist to return success/failure, handle missing playlists
- [ ] Design:
  ```python
  def assign_playlist(self, video_id: str, playlist_name: str) -> bool:
      """Assign video to playlist. Creates playlist if missing.
      Returns True on success, False on failure (logged)."""
  ```
- [ ] If playlist doesn't exist → create it (YouTube Studio "New playlist" flow)
- [ ] Pipeline should log playlist failures but NOT fail the upload
- [ ] SyncReport gets `playlist_failed: int` field
- [ ] Must NOT: Raise exception on playlist failure (fire-and-forget, logged)
- [ ] Acceptance:
  - Existing playlist → assigned → returns True
  - Missing playlist → created → assigned → returns True
  - Selector missing → returns False, warning logged
  - Pipeline counts playlist failures in SyncReport
  - Tests cover all 3 paths
- Parallelization: wave=5, blocks=[], blocked-by=[5.1, 2.1]
- Commit: `feat: playlist creation + bool return for assign_playlist`

---

## Wave 6 — Quality: Test Coverage

### TODO 6.1: CLI tests with CliRunner
- [ ] What: Test all CLI commands via click.testing.CliRunner
- [ ] File: tests/test_cli.py
- [ ] Scenarios:
  - upload --dry-run shows plan
  - upload with --limit passes value to pipeline
  - transcribe invokes fetch_transcripts
  - status formats output correctly
  - clean --dry-run shows eligible files
  - setup missing chrome → error
  - --verbose sets DEBUG logging
  - Unknown command → exit code 2
- [ ] Must NOT: Hit real YouTube (all adapters mocked)
- [ ] Acceptance: ≥8 test cases, all passing, mypy clean
- Parallelization: wave=6, blocks=[], blocked-by=[4.2]
- Commit: `test: CLI command tests via CliRunner`

### TODO 6.2: Pipeline transcript path tests
- [ ] What: Test fetch_transcripts() for all TranscriptStatus outcomes
- [ ] File: tests/test_pipeline.py (extend)
- [ ] Scenarios:
  - Successful fetch → DONE
  - TranscriptUnavailableError → UNAVAILABLE, not retried
  - TranscriptNotReadyError → stays PENDING
  - Generic exception → ERROR
  - --retry includes ERROR entries
  - --force re-fetches DONE entries
  - No primary account → error report
  - Transcriber not initialized → error report
- [ ] Acceptance: ≥8 new test cases in TestRecordingPipeline
- Parallelization: wave=6, blocks=[], blocked-by=[3.3]
- Commit: `test: pipeline transcript workflow coverage`

### TODO 6.3: Pipeline clean_synced tests
- [ ] What: Test clean eligibility logic exhaustively
- [ ] Scenarios per 4.1 acceptance criteria + edge cases:
  - Empty registry → nothing to clean
  - File not on disk → skipped (not error)
  - OSError on unlink → error reported, others still processed
- [ ] Acceptance: ≥6 test cases
- Parallelization: wave=6, blocks=[], blocked-by=[4.1]
- Commit: `test: clean_synced eligibility and edge cases`

---

## Wave 7 — Future Architecture: Storage Backend Abstraction

### TODO 7.1: StorageBackend protocol for multi-platform support
- [ ] What: Abstract YouTube-specific operations behind a protocol
- [ ] Design:
  ```python
  # domain/protocols.py (extend)
  class StorageBackend(Protocol):
      """A destination where videos can be stored and organized."""
      name: str
      def upload(self, path: Path, title: str) -> str: ...  # returns item_id
      def assign_collection(self, item_id: str, collection_name: str) -> bool: ...
      def delete_remote(self, item_id: str) -> bool: ...
      def health_check(self) -> bool: ...
  ```
- [ ] YouTubeBrowserAdapter already satisfies this with minor rename
- [ ] Future backends: S3Adapter, LocalNASAdapter, BackblazeAdapter
- [ ] Must NOT: Implement new backends yet — just define the port
- [ ] Must NOT: Break existing YouTube flow
- [ ] Acceptance:
  - Protocol defined and documented
  - YouTubeBrowserAdapter satisfies it (mypy confirms)
  - README "Architecture" section updated
- Parallelization: wave=7, blocks=[], blocked-by=[2.1]
- Commit: `feat: StorageBackend protocol for multi-platform extensibility`

### TODO 7.2: Health check command
- [ ] What: `yt-recorder health` verifies system readiness
- [ ] Checks:
  - Config file exists and parses
  - Each account's storage_state.json exists and has cookies
  - Chrome/Chromium found on system
  - Registry file parseable (if exists)
  - yt-dlp available in PATH
- [ ] Design: returns exit code 0 if all good, 1 if any check fails
- [ ] Must NOT: Make network calls (offline check only)
- [ ] Acceptance:
  - Missing config → clear error message
  - Missing chrome → clear error message
  - All good → ✅ per check
- Parallelization: wave=7, blocks=[], blocked-by=[2.3]
- Commit: `feat: health check command for system readiness verification`

---

## Wave 8 — Future Architecture: Cross-Device & Self-Host Prep

### TODO 8.1: Content-hash dedup field in registry
- [ ] What: Optional `content_hash` (SHA-256, first 16 chars) column in registry
- [ ] Why: Prevents duplicate uploads (galaxy.mp4 scenario), enables rename detection,
  enables cross-device dedup (same file on 2 machines = same hash)
- [ ] Design:
  ```python
  # models.py
  @dataclass(frozen=True)
  class RegistryEntry:
      ...
      content_hash: str | None = None  # SHA-256 prefix, computed pre-upload
  ```
- [ ] Pipeline: compute hash before upload, check registry for existing hash
- [ ] Registry: new optional column, backwards-compatible (missing = None)
- [ ] Must NOT: Break existing registries without hash column
- [ ] Acceptance:
  - Same file uploaded twice → second attempt skipped ("already uploaded, hash match")
  - Renamed file with same content → detected as duplicate
  - Old registry without hash column → loads fine (hash=None)
- Parallelization: wave=8, blocks=[], blocked-by=[3.2]
- Commit: `feat: content-hash dedup prevents duplicate uploads`

### TODO 8.2: Document self-hosting architecture
- [ ] What: docs/SELF_HOSTING.md explaining how to extend with custom backends
- [ ] Content:
  - StorageBackend protocol reference
  - Example: implementing a local NAS backend (pseudocode)
  - Config changes needed for multi-backend
  - Privacy guarantees: what data goes where
  - Registry sync options (git, syncthing, manual export/import)
- [ ] Must NOT: Implement actual backends — just document the extension point
- [ ] Acceptance: Document exists, references actual protocol types
- Parallelization: wave=8, blocks=[], blocked-by=[7.1]
- Commit: `docs: self-hosting architecture and extension guide`

---

## Wave 9 — Open-Source Posture

### TODO 9.1: SECURITY.md + privacy manifest
- [ ] What: Security policy and privacy documentation
- [ ] Content:
  ```markdown
  # Security Policy
  ## Reporting Vulnerabilities
  Email: [redacted] (or GitHub private advisory)
  ## What Data Leaves Your Machine
  - YouTube API calls (upload, transcript fetch) — via browser session
  - No telemetry, no analytics, no phone-home
  ## Credential Storage
  - storage_state.json: Playwright session (cookies + localStorage)
  - cookies.txt: Netscape format for yt-dlp
  - Both: 0600 permissions, never logged, .gitignore'd
  ```
- Parallelization: wave=9, blocks=[], blocked-by=[]
- Commit: `docs: SECURITY.md with privacy manifest`

### TODO 9.2: Issue templates (anti-AI-spam posture)
- [ ] What: GitHub issue templates that require effort, no PR template
- [ ] Files:
  - `.github/ISSUE_TEMPLATE/bug_report.yml` (structured form, required fields)
  - `.github/ISSUE_TEMPLATE/feature_request.yml` (requires use case, not solution)
  - `.github/ISSUE_TEMPLATE/config.yml` with `blank_issues_enabled: false`
  - NO pull_request_template.md (not accepting PRs initially)
- [ ] README badge: "Issues welcome · PRs paused"
- [ ] Acceptance:
  - Blank issues disabled
  - Bug template requires: version, OS, steps to reproduce, expected vs actual
  - Feature template requires: use case description, not implementation
- Parallelization: wave=9, blocks=[], blocked-by=[]
- Commit: `chore: issue templates, no-PR posture for AI spam prevention`

### TODO 9.3: README refresh with architecture diagram
- [ ] What: Update README to reflect new architecture, commands, and posture
- [ ] Additions:
  - Architecture diagram (text-based)
  - Full pipeline: `upload --keep → status → transcribe → clean`
  - TranscriptStatus explanation
  - "Not accepting PRs" notice with reasoning
  - Link to SECURITY.md
  - Link to docs/SELF_HOSTING.md
- Parallelization: wave=9, blocks=[], blocked-by=[8.2, 9.1]
- Commit: `docs: README refresh with architecture, full pipeline, posture`

---

## Dependency Graph

```
Wave 1: [1.1] → [1.2]
             ↘
Wave 2: [2.1] → [2.2]    [2.3]
           ↓
Wave 3: [3.1] → [3.2] → [3.3]
                            ↓
Wave 4:                  [4.1] → [4.2]
Wave 5: [5.1] → [5.2]
Wave 6: [6.1]  [6.2]  [6.3]        ← all unblocked by their feature wave
Wave 7: [7.1]  [7.2]
Wave 8: [8.1]  [8.2]
Wave 9: [9.1]  [9.2]  [9.3]

Critical path: 1.1 → 2.1 → 3.1 → 3.2 → 3.3 → 4.1 → 4.2
```

Waves 5-9 are parallelizable with waves 3-4 after Wave 2 completes.

## Success Criteria

1. **End-to-end pipeline works**: `upload --keep → status → transcribe → clean`
2. **No silent failures**: playlist warns are surfaced; transcript status is terminal
3. **No infinite loops**: UNAVAILABLE videos never retried unless --force
4. **Protocol-based**: swapping adapters requires zero pipeline changes
5. **Registry migration**: v1 registries auto-upgrade on first load
6. **Test coverage**: CLI, pipeline transcripts, and clean all tested
7. **Open-source ready**: SECURITY.md, issue templates, no-PR posture, self-host docs
8. **DRY**: zero duplicated functions, all selectors in constants.py

## Non-Obvious Design Decisions

### Why `str, Enum` for TranscriptStatus
Serializes directly to/from registry markdown. `TranscriptStatus("done")` round-trips.
No custom serializer needed. Also enables future values without migration.

### Why Protocols over ABC
Python Protocols use structural typing — existing Mock objects automatically satisfy them.
No test changes needed. ABCs would require explicit inheritance, breaking the mock pattern.

### Why content-hash is optional (Wave 8, not Wave 3)
Hashing large video files (500MB+) takes time. Making it optional means:
1. Existing registries without hash column still work
2. Users can opt in via `--hash` flag when uploading
3. Cross-device sync uses hash for dedup but doesn't require it

### Why no PR template
AI-generated PRs are the #1 spam vector for open-source in 2025-2026.
By not having a PR template and stating "PRs paused" in README, the project
signals it's not a target. Issues are still open for genuine users.

### Why markdown registry over SQLite
Git-diffable. Human-readable. Cross-device sync via `git add registry.md`.
SQLite would be faster for 10k+ entries but defeats the transparency goal.
Migration path exists (RegistryStore protocol) if scale demands it.

### Why assign_playlist returns bool, not raises
Upload is the valuable operation. Playlist is organization. Failing a $5 operation
(upload) because a $0.01 operation (playlist) had a stale selector is wrong.
Log it, count it, surface it — but don't lose the upload.
